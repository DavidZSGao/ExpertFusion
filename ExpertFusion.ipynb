{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExpertFusion: Mixture of Experts Model\n",
    "\n",
    "This notebook implements a Mixture of Experts (MoE) model for financial market analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup & Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import openai\n",
    "import sqlite3\n",
    "import yfinance as yf\n",
    "import wrds\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "import json\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Global variables\n",
    "DB_FILE = \"sp500_daily_news.db\"  # Update if needed\n",
    "_LAST_QUERY_TIME = 0\n",
    "_MIN_QUERY_INTERVAL = 1.0  # seconds\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable\")\n",
    "\n",
    "client = openai.OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GPT Factor & Explanation\n",
    "\n",
    "Functions for generating factor scores and explanations using GPT-4o-mini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt_factor_and_expl(system_msg: str, user_msg: str) -> Tuple[float, str]:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": user_msg}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=150\n",
    "        )\n",
    "        text = response.choices[0].message.content\n",
    "        # Debug: print raw GPT response\n",
    "        print(f\"[DEBUG] Raw GPT response:\\n{text}\\n\")\n",
    "        factor_match = re.search(r\"Factor:\\s*(-?\\d*\\.?\\d+)\", text)\n",
    "        expl_match = re.search(r\"Explanation:\\s*(.+)\", text, re.DOTALL)\n",
    "        factor = float(factor_match.group(1)) if factor_match else 0.0\n",
    "        explanation = expl_match.group(1).strip() if expl_match else \"\"\n",
    "        if not explanation:\n",
    "            print(\"[WARN] No explanation parsed from GPT response.\")\n",
    "        return factor, explanation\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] GPT call failed: {e}\")\n",
    "        return 0.0, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Database Access & WRDS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrds_connection():\n",
    "    try:\n",
    "        wrds_username = os.getenv('WRDS_USERNAME')\n",
    "        if wrds_username:\n",
    "            print(f\"[INFO] Using WRDS username: {wrds_username}\")\n",
    "            db = wrds.Connection(wrds_username=wrds_username)\n",
    "        else:\n",
    "            db = wrds.Connection()\n",
    "        print(\"[INFO] Connected to WRDS.\")\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to connect to WRDS: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_tickers_from_db(skip_wrds=False) -> List[str]:\n",
    "    try:\n",
    "        SP500_LIST_FILE = \"sp500_list_wiki.json\"\n",
    "        if os.path.exists(SP500_LIST_FILE):\n",
    "            with open(SP500_LIST_FILE, 'r') as f:\n",
    "                sp500_data = json.load(f)\n",
    "            tickers = sp500_data['tickers']\n",
    "            fetch_time = sp500_data['fetch_time']\n",
    "            print(f\"[INFO] Loaded {len(tickers)} tickers from {SP500_LIST_FILE} (fetched at {fetch_time})\")\n",
    "            if skip_wrds:\n",
    "                print(\"[INFO] Skipping WRDS connection, using tickers directly from file\")\n",
    "                return tickers\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"{SP500_LIST_FILE} not found. Please run fetch_sp500_list.py with VPN first.\")\n",
    "        db = get_wrds_connection()\n",
    "        if not db:\n",
    "            return tickers\n",
    "        print(\"[INFO] Getting ticker mappings from WRDS...\")\n",
    "        today = pd.Timestamp.today()\n",
    "        today_str = today.strftime('%Y-%m-%d')\n",
    "        ticker_query = f\"\"\"\n",
    "            SELECT DISTINCT n.permno, n.ticker, n.namedt, n.nameendt\n",
    "            FROM crsp.msenames n\n",
    "            WHERE n.ticker IN ({','.join(map(lambda x: f\"'{x}'\", tickers))})\n",
    "              AND n.namedt <= '{today_str}'\n",
    "              AND (n.nameendt >= '{today_str}' OR n.nameendt IS NULL)\n",
    "            ORDER BY n.namedt DESC\n",
    "        \"\"\"\n",
    "        ticker_df = db.raw_sql(ticker_query, date_cols=['namedt', 'nameendt'])\n",
    "        print(f\"[INFO] Retrieved {len(ticker_df)} ticker mappings from msenames\")\n",
    "        if ticker_df.empty:\n",
    "            raise ValueError(\"No ticker mappings found in WRDS.\")\n",
    "        print(\"[INFO] Sample of ticker mappings:\")\n",
    "        print(ticker_df.head())\n",
    "        ticker_df = ticker_df.sort_values('namedt', ascending=False).drop_duplicates('permno')\n",
    "        mapped_tickers = ticker_df['ticker'].unique().tolist()\n",
    "        print(f\"[INFO] Retrieved {len(mapped_tickers)} unique tickers from WRDS\")\n",
    "        return mapped_tickers\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get tickers from WRDS: {e}\")\n",
    "        raise\n",
    "\n",
    "def rate_limited_query(query: str, date_cols=None, params=None) -> pd.DataFrame:\n",
    "    global _LAST_QUERY_TIME\n",
    "    current_time = time.time()\n",
    "    time_since_last = current_time - _LAST_QUERY_TIME\n",
    "    if time_since_last < _MIN_QUERY_INTERVAL:\n",
    "        time.sleep(_MIN_QUERY_INTERVAL - time_since_last)\n",
    "    db = get_wrds_connection()\n",
    "    if not db:\n",
    "        raise ConnectionError(\"No WRDS connection available.\")\n",
    "    try:\n",
    "        result = db.raw_sql(query, params=params, date_cols=date_cols)\n",
    "        _LAST_QUERY_TIME = time.time()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] WRDS query failed: {e}\")\n",
    "        raise\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def get_fundamentals_wrds(ticker: str, date: datetime.date) -> Dict:\n",
    "    try:\n",
    "        wrds_data = load_cached_data(\"wrds\", date=date.strftime('%Y-%m-%d'))\n",
    "        if wrds_data and ticker in wrds_data:\n",
    "            return wrds_data[ticker].get(date.isoformat(), {})\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get fundamental data: {e}\")\n",
    "        raise\n",
    "\n",
    "def cache_wrds_data(date: str) -> Dict:\n",
    "    print(\"[INFO] Caching WRDS data...\")\n",
    "    data = {}\n",
    "    try:\n",
    "        db = get_wrds_connection()\n",
    "        if not db:\n",
    "            raise ConnectionError(\"No WRDS connection available for caching.\")\n",
    "        tickers = get_tickers_from_db()\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                date_obj = datetime.datetime.strptime(date, \"%Y-%m-%d\").date()\n",
    "                fundamentals = get_fundamentals_wrds(ticker, date_obj)\n",
    "                if fundamentals:\n",
    "                    data[ticker] = {date: fundamentals}\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to get WRDS data for {ticker}: {e}\")\n",
    "                continue\n",
    "        if data:\n",
    "            print(f\"[INFO] Saving WRDS data for {len(data)} tickers\")\n",
    "            with open(f'wrds_cache_{date}.pkl', 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "        else:\n",
    "            print(\"[WARN] No WRDS data to cache\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to cache WRDS data: {e}\")\n",
    "        raise\n",
    "\n",
    "def cache_yfinance_data(tickers: List[str], start_date: str, end_date: str) -> Dict:\n",
    "    print(\"[INFO] Caching yfinance data...\")\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            df = get_market_data(ticker, start_date, end_date)\n",
    "            if not df.empty:\n",
    "                data[ticker] = df\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] yfinance caching failed for {ticker}: {e}\")\n",
    "            continue\n",
    "    if data:\n",
    "        print(f\"[INFO] Saving yfinance data for {len(data)} tickers\")\n",
    "        with open(f'yfinance_cache_{start_date}_{end_date}.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "    else:\n",
    "        print(\"[WARN] No yfinance data to cache\")\n",
    "    return data\n",
    "\n",
    "def load_cached_data(data_type: str, date: str = None, start_date: str = None, end_date: str = None):\n",
    "    if data_type == \"wrds\":\n",
    "        cache_file = f'wrds_cache_{date}.pkl'\n",
    "    elif data_type == \"yfinance\":\n",
    "        cache_file = f'yfinance_cache_{start_date}_{end_date}.pkl'\n",
    "    elif data_type == \"macro_uncertainty\":\n",
    "        cache_file = f'macro_uncertainty_cache_{start_date}_{end_date}.pkl'\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data type: {data_type}\")\n",
    "    if not os.path.exists(cache_file):\n",
    "        raise FileNotFoundError(f\"Cache file not found: {cache_file}\")\n",
    "    try:\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"[INFO] Loaded {data_type} data from cache: {cache_file}\")\n",
    "        if data_type == \"macro_uncertainty\":\n",
    "            if not isinstance(data.index, (pd.DatetimeIndex, pd.PeriodIndex)):\n",
    "                if isinstance(data.index, pd.RangeIndex):\n",
    "                    print(f\"[DEBUG] Reconstructing macro_uncertainty index using start_date {start_date} and end_date {end_date}\")\n",
    "                    new_index = pd.date_range(start=start_date, end=end_date, freq='D').date\n",
    "                    data.index = new_index\n",
    "                else:\n",
    "                    raise TypeError(f\"Unexpected macro_df index type: {type(data.index)}. Full index: {data.index}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {data_type} cache: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_news_from_db(ticker: str, date: datetime.date) -> List[str]:\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_FILE)\n",
    "        c = conn.cursor()\n",
    "        c.execute(\"SELECT headlines FROM news_storage WHERE ticker = ? AND date = ?\", (ticker, date.isoformat()))\n",
    "        result = c.fetchone()\n",
    "        conn.close()\n",
    "        if result:\n",
    "            return result[0].split(\" || \")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get news from DB: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_date_range_from_db() -> Tuple[datetime.date, datetime.date]:\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_FILE)\n",
    "        c = conn.cursor()\n",
    "        c.execute(\"SELECT MIN(date), MAX(date) FROM news_storage\")\n",
    "        start_date_str, end_date_str = c.fetchone()\n",
    "        conn.close()\n",
    "        if start_date_str and end_date_str:\n",
    "            return (datetime.date.fromisoformat(start_date_str),\n",
    "                    datetime.date.fromisoformat(end_date_str))\n",
    "        end_date = datetime.date.today()\n",
    "        start_date = end_date - datetime.timedelta(days=30)\n",
    "        return start_date, end_date\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get date range from DB: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_fundamentals_av(ticker: str) -> Dict:\n",
    "    try:\n",
    "        db = get_wrds_connection()\n",
    "        if not db:\n",
    "            raise ConnectionError(\"No WRDS connection available.\")\n",
    "        print(f\"[INFO] Querying fundamental data for {ticker} from WRDS...\")\n",
    "        query = \"\"\"\n",
    "            SELECT c.gvkey, c.datadate, \n",
    "                   c.prccm/c.ceqq AS pe_ratio,\n",
    "                   c.ibq/c.ceqq AS roe\n",
    "            FROM comp.fundq AS c\n",
    "            JOIN crsp.ccmxpf_linktable AS l ON c.gvkey = l.gvkey\n",
    "            JOIN crsp.dsenames AS d ON l.lpermno = d.permno\n",
    "            WHERE d.ticker = %s AND c.ceqq > 0\n",
    "            ORDER BY c.datadate DESC\n",
    "            LIMIT 1\n",
    "        \"\"\"\n",
    "        fund_data = rate_limited_query(query, date_cols=['datadate'], params=(ticker,))\n",
    "        if fund_data.empty:\n",
    "            raise ValueError(f\"No fundamental data found for {ticker}\")\n",
    "        result = {\"pe_ratio\": str(fund_data.iloc[0]['pe_ratio']),\n",
    "                  \"roe\": str(fund_data.iloc[0]['roe'])}\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get fundamental data from WRDS: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FOMC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOMCDateBasedFetcher:\n",
    "    BASE_URL = \"https://www.federalreserve.gov\"\n",
    "    CAL_URL  = f\"{BASE_URL}/monetarypolicy/fomccalendars.htm\"\n",
    "    def __init__(self):\n",
    "        self.all_meetings = self.parse_calendars_for_all_years()\n",
    "        self.all_meetings.sort(key=lambda x: x[\"end_date\"])\n",
    "        for m in self.all_meetings:\n",
    "            m[\"statement_text\"] = None\n",
    "    def fetch_fomc_calendars_page(self) -> str:\n",
    "        try:\n",
    "            resp = requests.get(self.CAL_URL)\n",
    "            if not resp.ok:\n",
    "                raise ValueError(f\"FOMC calendars fetch failed with HTTP {resp.status_code}\")\n",
    "            return resp.text\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] fetch_fomc_calendars_page =>\", e)\n",
    "            raise\n",
    "    def fetch_fomc_statement_text(self, link_url: str) -> str:\n",
    "        try:\n",
    "            resp = requests.get(link_url)\n",
    "            if not resp.ok:\n",
    "                raise ValueError(f\"Statement fetch failed with HTTP {resp.status_code}\")\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            main_div = soup.find(\"div\", class_=\"col-xs-12 col-sm-8 col-md-8\")\n",
    "            if not main_div:\n",
    "                raise ValueError(\"FOMC statement div not found\")\n",
    "            return main_div.get_text(separator=\"\\n\", strip=True)\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] fetch_fomc_statement_text =>\", e)\n",
    "            raise\n",
    "    def parse_meeting_divs_for_year(self, html: str, year: int) -> list:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        meeting_divs = soup.find_all(\"div\", class_=re.compile(r\"fomc-meeting\"))\n",
    "        results = []\n",
    "        for div in meeting_divs:\n",
    "            raw_text = div.get_text(\" \", strip=True)\n",
    "            m = re.search(\n",
    "                r\"(January|February|March|April|May|June|July|August|September|October|November|December)\\s+(\\d{1,2})(?:-(\\d{1,2}))?\",\n",
    "                raw_text\n",
    "            )\n",
    "            if not m:\n",
    "                continue\n",
    "            month_str = m.group(1)\n",
    "            day1 = int(m.group(2))\n",
    "            day2 = m.group(3)\n",
    "            month_map = {'January': 1, 'February': 2, 'March': 3, 'April': 4,\n",
    "                         'May': 5, 'June': 6, 'July': 7, 'August': 8,\n",
    "                         'September': 9, 'October': 10, 'November': 11, 'December': 12}\n",
    "            month_int = month_map.get(month_str, 1)\n",
    "            start_dt = datetime.date(year, month_int, day1)\n",
    "            end_dt   = datetime.date(year, month_int, int(day2)) if day2 else start_dt\n",
    "            link_tag = div.find(\"a\", href=re.compile(r\"/newsevents/pressreleases/monetary.*\\.htm\"))\n",
    "            link_href = \"\"\n",
    "            if link_tag and link_tag.has_attr(\"href\"):\n",
    "                link_href = link_tag[\"href\"]\n",
    "                if link_href.startswith(\"/\"):\n",
    "                    link_href = self.BASE_URL + link_href\n",
    "            snippet = raw_text[:120]\n",
    "            results.append({\"start_date\": start_dt, \"end_date\": end_dt, \"link\": link_href, \"snippet\": snippet})\n",
    "        return results\n",
    "    def parse_calendars_for_all_years(self) -> list:\n",
    "        out = []\n",
    "        full_html = self.fetch_fomc_calendars_page()\n",
    "        if not full_html:\n",
    "            raise ValueError(\"No HTML received from FOMC calendars page.\")\n",
    "        soup = BeautifulSoup(full_html, \"html.parser\")\n",
    "        panels = soup.find_all(\"div\", class_=\"panel panel-default\")\n",
    "        for panel in panels:\n",
    "            heading = panel.find(\"div\", class_=\"panel-heading\")\n",
    "            if not heading:\n",
    "                continue\n",
    "            heading_txt = heading.get_text(\" \", strip=True)\n",
    "            m = re.search(r\"(\\d{4}) FOMC Meetings\", heading_txt)\n",
    "            if not m:\n",
    "                continue\n",
    "            year = int(m.group(1))\n",
    "            panel_html = str(panel)\n",
    "            subset = self.parse_meeting_divs_for_year(panel_html, year)\n",
    "            out.extend(subset)\n",
    "        return out\n",
    "    def get_most_recent_fomc_for(self, date_val: datetime.date) -> str:\n",
    "        valid = [m for m in self.all_meetings if m[\"end_date\"] <= date_val]\n",
    "        if not valid:\n",
    "            raise ValueError(f\"No FOMC meeting found before {date_val}\")\n",
    "        valid_desc = sorted(valid, key=lambda x: x[\"end_date\"], reverse=True)\n",
    "        for meeting in valid_desc:\n",
    "            link = meeting[\"link\"]\n",
    "            if not link:\n",
    "                continue\n",
    "            if meeting[\"statement_text\"]:\n",
    "                return meeting[\"statement_text\"]\n",
    "            text = self.fetch_fomc_statement_text(link)\n",
    "            if text:\n",
    "                meeting[\"statement_text\"] = text\n",
    "                return text\n",
    "        raise ValueError(f\"No FOMC statement could be retrieved for date {date_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Market Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\"_\".join(map(str, c)) if isinstance(c, tuple) else str(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def rename_yf_columns(df: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "    rename_dict = {}\n",
    "    suffix = f\"_{ticker}\"\n",
    "    for c in df.columns:\n",
    "        if c.endswith(suffix):\n",
    "            newc = c.replace(suffix, \"\")\n",
    "            rename_dict[c] = newc\n",
    "    if rename_dict:\n",
    "        df.rename(columns=rename_dict, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_market_data(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        yf_ticker = ticker\n",
    "        if any(x in ticker for x in ['.', '-', '/']):\n",
    "            yf_ticker = ticker.replace('-', '.').replace('/', '.')\n",
    "        print(f\"[INFO] Getting market data for {ticker} from {start_date} to {end_date}\")\n",
    "        df = yf.download(yf_ticker, start=start_date, end=end_date, progress=False)\n",
    "        if df.empty:\n",
    "            print(f\"[WARN] No data returned for {ticker}\")\n",
    "            return pd.DataFrame()\n",
    "        if isinstance(df.columns, pd.MultiIndex):\n",
    "            df.columns = [col[0] + ' ' + col[1] if col[1] else col[0] for col in df.columns]\n",
    "        col_map = {'Adj Close': 'close', 'Adj_Close': 'close', 'adj_close': 'close',\n",
    "                   'Volume': 'volume', 'volume': 'volume'}\n",
    "        df = df.rename(columns=col_map)\n",
    "        df = df.reset_index()\n",
    "        if 'index' in df.columns:\n",
    "            df = df.rename(columns={'index': 'date'})\n",
    "        elif 'Date' in df.columns:\n",
    "            df = df.rename(columns={'Date': 'date'})\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to get market data for {ticker}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Domain Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroExpert:\n",
    "    \"\"\"Expert for macro-economic factors.\"\"\"\n",
    "    def __init__(self, macro_df: pd.DataFrame, fomcFetcher: FOMCDateBasedFetcher):\n",
    "        self.macro_df = macro_df\n",
    "        self.fomcFetcher = fomcFetcher\n",
    "    def produce_factor(self, dt_val: datetime.date, ticker: str) -> float:\n",
    "        try:\n",
    "            if self.macro_df is None or self.macro_df.empty:\n",
    "                raise ValueError(\"Macro data is missing or empty.\")\n",
    "            first_index = self.macro_df.index[0]\n",
    "            print(f\"[DEBUG] Macro data index type: {type(first_index)}, value: {first_index}\")\n",
    "            if isinstance(first_index, pd.Timestamp):\n",
    "                hist_data = self.macro_df[self.macro_df.index.map(lambda x: x.date()) <= dt_val]\n",
    "            elif isinstance(first_index, datetime.date):\n",
    "                hist_data = self.macro_df[self.macro_df.index <= dt_val]\n",
    "            else:\n",
    "                raise TypeError(f\"Unexpected macro_df index type: {type(first_index)}. Full index: {self.macro_df.index}\")\n",
    "            if len(hist_data) < 2:\n",
    "                raise ValueError(f\"Not enough macro data history before {dt_val}.\")\n",
    "            current = float(hist_data.iloc[-1]['value'])\n",
    "            previous = float(hist_data.iloc[-2]['value'])\n",
    "            fomc_text = self.fomcFetcher.get_most_recent_fomc_for(dt_val)\n",
    "            fomc_snippet = fomc_text[:200] if fomc_text else \"No FOMC statement\"\n",
    "            sys = (\n",
    "                \"You are a macro analysis expert. Consider how GDP and monetary policy specifically impact this company:\\n\"\n",
    "                \"1. Analyze the company's industry, business model, and macro sensitivities, considering both short and long-term impacts.\\n\"\n",
    "                \"2. Evaluate how current GDP and FOMC stance affect this specific business.\\n\"\n",
    "                \"3. Consider factors like discretionary vs essential products, consumer financing, interest rates, \"\n",
    "                \"supply chain, global trade, currency exposure, and international revenue.\\n\"\n",
    "                \"4. Based on these factors, determine whether the overall macro environment is bullish or bearish for this company.\\n\"\n",
    "                \"5. Output a factor between -1 (very negative) and +1 (very positive), reflecting the bullish or bearish nature.\\n\"\n",
    "                \"Format => Factor: <float>\\n\"\n",
    "                \"Explanation: <text>\"\n",
    "            )\n",
    "            usr = f\"Company: {ticker}\\nGDP={current:.2f} vs {previous:.2f}\\nFOMC: {fomc_snippet}\"\n",
    "            fac, _ = call_gpt_factor_and_expl(sys, usr)\n",
    "            return fac\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] MacroExpert failed: {e}\")\n",
    "            raise\n",
    "\n",
    "class FundamentalExpert:\n",
    "    \"\"\"Expert for fundamental analysis.\"\"\"\n",
    "    def produce_factor(self, fundamentals: Dict) -> float:\n",
    "        try:\n",
    "            if not fundamentals:\n",
    "                raise ValueError(\"No fundamental data provided.\")\n",
    "            pe = fundamentals.get('pe', '')\n",
    "            roe = fundamentals.get('roe', '')\n",
    "            sys = (\n",
    "                \"You are an expert in fundamental analysis. Evaluate the company's financial health based on the following:\\n\"\n",
    "                \"1. P/E ratio: Is the company undervalued (bullish) or overvalued (bearish)? Consider industry averages and market conditions.\\n\"\n",
    "                \"2. ROE: Is the company generating strong returns on equity? Does it indicate a healthy business model?\\n\"\n",
    "                \"3. Based on these financial metrics, determine if the stock is bullish or bearish.\\n\"\n",
    "                \"Output a factor between -1 (very negative) and +1 (very positive), with an explanation based on the metrics.\\n\"\n",
    "                \"Format => Factor: <float>\\n\"\n",
    "                \"Explanation: <text>\"\n",
    "            )\n",
    "            usr = f\"P/E={pe}, ROE={roe}\"\n",
    "            fac, _ = call_gpt_factor_and_expl(sys, usr)\n",
    "            return fac\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] FundamentalExpert failed: {e}\")\n",
    "            raise\n",
    "\n",
    "class NewsExpert:\n",
    "    def produce_factor(self, ticker: str, date: datetime.date) -> float:\n",
    "        try:\n",
    "            headlines = get_news_from_db(ticker, date)\n",
    "            if not headlines:\n",
    "                print(f\"[WARN] No news found for {ticker} on {date}. Returning default factor of 0.0.\")\n",
    "                return 0.0\n",
    "            short = \"\\n\".join(headlines[:3])\n",
    "            sys = (\n",
    "                \"You are an expert in news sentiment analysis. Consider the news headlines for the company and evaluate:\\n\"\n",
    "                \"1. Are the headlines generally positive, neutral, or negative in terms of market sentiment?\\n\"\n",
    "                \"2. How does the news affect investor sentiment towards the company? Is the news likely to drive the stock price up or down?\\n\"\n",
    "                \"3. Based on your analysis, determine whether the overall sentiment is bullish or bearish for the stock.\\n\"\n",
    "                \"4. Output a factor between -1 (very negative) and +1 (very positive) reflecting the sentiment.\\n\"\n",
    "                \"Format => Factor: <float>\\n\"\n",
    "                \"Explanation: <text>\"\n",
    "            )\n",
    "            usr = f\"{short}\"\n",
    "            fac, _ = call_gpt_factor_and_expl(sys, usr)\n",
    "            return fac\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] NewsExpert failed: {e}\")\n",
    "            raise\n",
    "\n",
    "class TechnicalExpert:\n",
    "    def produce_factor(self, df: pd.DataFrame, global_idx: int) -> float:\n",
    "        try:\n",
    "            # Filter for the current ticker and reset index.\n",
    "            row = df.iloc[global_idx]\n",
    "            ticker = row['ticker']\n",
    "            group = df[df['ticker'] == ticker].reset_index(drop=True)\n",
    "            positions = group.index[group['date'] == row['date']]\n",
    "            if len(positions) == 0:\n",
    "                raise ValueError(f\"Could not find row for ticker {ticker} on {row['date']} in its group.\")\n",
    "            pos = positions[0]\n",
    "            if pos < 4:\n",
    "                print(f\"[WARN] Not enough bars for technical analysis for ticker {ticker} on {row['date']}. Returning default factor of 0.0.\")\n",
    "                return 0.0\n",
    "            pct_changes = group['close'].pct_change().values\n",
    "            w5 = pct_changes[pos-4:pos+1]\n",
    "            s = \", \".join(f\"{x:.4f}\" for x in w5 if not np.isnan(x))\n",
    "            sys = (\n",
    "                \"You are an expert in technical analysis. Evaluate the following technical indicators for the stock:\\n\"\n",
    "                \"1. How do recent price and volume patterns reflect the overall trend in the market?\\n\"\n",
    "                \"2. Is the stock showing bullish or bearish signals based on technical indicators such as moving averages, momentum, or volume trends?\\n\"\n",
    "                \"3. Based on this analysis, is the stock in a bullish or bearish trend, and what is your overall sentiment?\\n\"\n",
    "                \"Output a factor between -1 (very negative) and +1 (very positive), representing the technical sentiment.\\n\"\n",
    "                \"Format => Factor: <float>\\n\"\n",
    "                \"Explanation: <text>\"\n",
    "            )\n",
    "            usr = f\"Last5 returns => {s}\"\n",
    "            fac, _ = call_gpt_factor_and_expl(sys, usr)\n",
    "            return fac\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] TechnicalExpert failed: {e}\")\n",
    "            raise\n",
    "\n",
    "class RiskExpert:\n",
    "    def produce_factor(self, df: pd.DataFrame, global_idx: int) -> float:\n",
    "        try:\n",
    "            if global_idx < 5 or 'close' not in df.columns:\n",
    "                raise ValueError(\"Not enough data for risk analysis.\")\n",
    "            rets = df['close'].pct_change().dropna().values\n",
    "            w_5 = rets[global_idx-4:global_idx+1]\n",
    "            stv = np.nanstd(w_5)\n",
    "            sys = (\n",
    "                \"You are an expert in risk analysis. Evaluate the following risk metrics for the stock:\\n\"\n",
    "                \"1. What does the recent volatility and beta tell you about the stock's risk profile?\\n\"\n",
    "                \"2. Is the stock showing signs of higher volatility (bearish) or stability (bullish)? Consider if the risk is manageable or excessive.\\n\"\n",
    "                \"3. Based on these risk factors, do you consider the stock to be a higher or lower risk, and how does this affect its bullish or bearish nature?\\n\"\n",
    "                \"4. Output a factor between -1 (very negative) and +1 (very positive), reflecting the risk sentiment.\\n\"\n",
    "                \"Format => Factor: <float>\\n\"\n",
    "                \"Explanation: <text>\"\n",
    "            )\n",
    "            usr = f\"5-day std dev = {stv:.4f}\"\n",
    "            fac, _ = call_gpt_factor_and_expl(sys, usr)\n",
    "            return fac\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] RiskExpert failed: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. MoE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, num_experts=5):\n",
    "        super().__init__()\n",
    "        self.input_norm = nn.BatchNorm1d(num_experts)\n",
    "        self.fc = nn.Linear(num_experts, num_experts)\n",
    "    def forward(self, expert_preds: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_norm(expert_preds)\n",
    "        logits = self.fc(x)\n",
    "        weights = F.softmax(logits / 0.5, dim=1)\n",
    "        return weights\n",
    "\n",
    "class MoEModel(nn.Module):\n",
    "    def __init__(self, num_experts=5):\n",
    "        super().__init__()\n",
    "        self.gate = GatingNetwork(num_experts)\n",
    "        self.scale = nn.Parameter(torch.tensor([0.05]))\n",
    "    def forward(self, expert_preds: torch.Tensor) -> torch.Tensor:\n",
    "        weights = self.gate(expert_preds)\n",
    "        combined_pred = (expert_preds * weights).sum(dim=1)\n",
    "        return torch.tanh(combined_pred) * self.scale\n",
    "\n",
    "def combined_loss(pred: torch.Tensor, target: torch.Tensor, model: nn.Module) -> torch.Tensor:\n",
    "    huber = F.huber_loss(pred, target, reduction='mean', delta=0.1)\n",
    "    pred_direction = torch.sign(pred)\n",
    "    target_direction = torch.sign(target)\n",
    "    direction_loss = -torch.mean(pred_direction * target_direction)\n",
    "    scale_reg = 0.1 * model.scale**2\n",
    "    total_loss = huber + 0.3 * direction_loss + scale_reg\n",
    "    return total_loss\n",
    "\n",
    "def train_moe_model(recs: List[Dict], epochs=200):\n",
    "    X = torch.tensor([r['expert_predictions'] for r in recs], dtype=torch.float32)\n",
    "    y = torch.tensor([r['target'] for r in recs], dtype=torch.float32)\n",
    "    dataset = torch.utils.data.TensorDataset(X, y)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "    model = MoEModel(num_experts=X.shape[1])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = combined_loss(outputs, batch_y, model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += combined_loss(outputs, batch_y, model).item()\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "        scheduler.step(val_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss/len(train_loader):.4f}, Val Loss = {val_loss/len(val_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "def process_single_prediction(prediction: float) -> float:\n",
    "    return np.tanh(prediction) * 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(tickers: List[str] = None, start_date: str = None, end_date: str = None, fomcFetcher=None):\n",
    "    print(\"[INFO] Building dataset...\")\n",
    "    print(\"[INFO] Loading tickers from file...\")\n",
    "    with open(\"sp500_list_wiki.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        tickers = data[\"tickers\"]\n",
    "        print(f\"[INFO] Loaded {len(tickers)} tickers from sp500_list_wiki.json (fetched at {data['fetch_time']})\")\n",
    "    wrds_data = load_cached_data(\"wrds\", date=start_date)\n",
    "    yf_data = load_cached_data(\"yfinance\", start_date=start_date, end_date=end_date)\n",
    "    macro_data = load_cached_data(\"macro_uncertainty\", start_date=start_date, end_date=end_date)\n",
    "    if fomcFetcher is None:\n",
    "        fomcFetcher = FOMCDateBasedFetcher()\n",
    "    dfs = []\n",
    "    for ticker in yf_data:\n",
    "        df = yf_data[ticker].copy()\n",
    "        if 'close' not in df.columns:\n",
    "            if 'Close' in df.columns:\n",
    "                df = df.rename(columns={'Close': 'close'})\n",
    "            elif 'Adj Close' in df.columns:\n",
    "                df = df.rename(columns={'Adj Close': 'close'})\n",
    "        if 'volume' not in df.columns:\n",
    "            if 'Volume' in df.columns:\n",
    "                df = df.rename(columns={'Volume': 'volume'})\n",
    "        df['ticker'] = ticker\n",
    "        df['date'] = pd.to_datetime(df.index).date\n",
    "        df['pe_ratio'] = df['date'].apply(lambda x: wrds_data.get(ticker, {}).get(x.strftime('%Y-%m-%d'), {}))\n",
    "        df['news'] = df['date'].apply(lambda x: get_news_from_db(ticker, x))\n",
    "        if isinstance(macro_data, pd.DataFrame) and not macro_data.empty:\n",
    "            df['macro_data'] = df['date'].apply(lambda x: macro_data.loc[pd.Timestamp(x).strftime('%Y-%m-%d')]['value']\n",
    "                                                 if pd.Timestamp(x).strftime('%Y-%m-%d') in macro_data.index\n",
    "                                                 else macro_data['value'].iloc[-1])\n",
    "        else:\n",
    "            df['macro_data'] = 0\n",
    "        if fomcFetcher is not None:\n",
    "            df['fomc'] = df['date'].apply(lambda x: fomcFetcher.get_most_recent_fomc_for(x))\n",
    "        else:\n",
    "            df['fomc'] = ''\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(\"\\n[DEBUG] DataFrame info:\")\n",
    "    print(combined_df.info())\n",
    "    print(\"\\n[DEBUG] First few rows:\")\n",
    "    print(combined_df.head())\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Compare MoE vs Single GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_based_train_test_split(rows: List[Dict], train_ratio=0.7) -> Tuple[List[Dict], List[Dict]]:\n",
    "    df = pd.DataFrame(rows)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    unique_dates = df['date'].unique()\n",
    "    unique_dates.sort()\n",
    "    n_train_dates = int(len(unique_dates) * train_ratio)\n",
    "    train_dates = unique_dates[:n_train_dates]\n",
    "    test_dates = unique_dates[n_train_dates:]\n",
    "    print(f\"[INFO] Train dates: {train_dates[0]} to {train_dates[-1]}\")\n",
    "    print(f\"[INFO] Test dates: {test_dates[0]} to {test_dates[-1]}\")\n",
    "    train_df = df[df['date'].isin(train_dates)]\n",
    "    test_df = df[df['date'].isin(test_dates)]\n",
    "    return train_df.to_dict('records'), test_df.to_dict('records')\n",
    "\n",
    "def compare_moe_vs_single(records: List[Dict]):\n",
    "    actual = np.array([r['true_return'] for r in records])\n",
    "    moe_pred = np.array([r['moe_prediction'] for r in records])\n",
    "    gpt_pred = np.array([r['single_prediction'] for r in records])\n",
    "    moe_rmse = np.sqrt(mean_squared_error(actual, moe_pred))\n",
    "    gpt_rmse = np.sqrt(mean_squared_error(actual, gpt_pred))\n",
    "    moe_mae = mean_absolute_error(actual, moe_pred)\n",
    "    gpt_mae = mean_absolute_error(actual, gpt_pred)\n",
    "    moe_dir = np.mean((actual > 0) == (moe_pred > 0))\n",
    "    gpt_dir = np.mean((actual > 0) == (gpt_pred > 0))\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(\"\\nRoot Mean Squared Error (RMSE):\")\n",
    "    print(f\"MoE Model: {moe_rmse:.4f}\")\n",
    "    print(f\"GPT Model: {gpt_rmse:.4f}\")\n",
    "    print(f\"Improvement: {((gpt_rmse - moe_rmse) / gpt_rmse * 100):.2f}%\")\n",
    "    print(\"\\nMean Absolute Error (MAE):\")\n",
    "    print(f\"MoE Model: {moe_mae:.4f}\")\n",
    "    print(f\"GPT Model: {gpt_mae:.4f}\")\n",
    "    print(f\"Improvement: {((gpt_mae - moe_mae) / gpt_mae * 100):.2f}%\")\n",
    "    print(\"\\nDirectional Accuracy:\")\n",
    "    print(f\"MoE Model: {moe_dir:.2%}\")\n",
    "    print(f\"GPT Model: {gpt_dir:.2%}\")\n",
    "    print(f\"Improvement: {((moe_dir - gpt_dir) / gpt_dir * 100):.2f}%\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(actual, label='Actual Returns', alpha=0.5)\n",
    "    plt.plot(moe_pred, label='MoE Predictions', alpha=0.5)\n",
    "    plt.plot(gpt_pred, label='GPT Predictions', alpha=0.5)\n",
    "    plt.title('Model Predictions vs Actual Returns')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df: pd.DataFrame, start_date: str, end_date: str):\n",
    "    print(\"[INFO] Running model...\")\n",
    "    \n",
    "    # Dataset Info\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(f\"Number of stocks: {len(df['ticker'].unique())}\")\n",
    "    print(\"\\nSample of data:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nColumns:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    # Rename columns if necessary\n",
    "    if 'close' not in df.columns:\n",
    "        if 'Close' in df.columns:\n",
    "            df = df.rename(columns={'Close': 'close'})\n",
    "        elif 'Adj Close' in df.columns:\n",
    "            df = df.rename(columns={'Adj Close': 'close'})\n",
    "    if 'volume' not in df.columns:\n",
    "        if 'Volume' in df.columns:\n",
    "            df = df.rename(columns={'Volume': 'volume'})\n",
    "    \n",
    "    # Debug: Calculating returns\n",
    "    print(\"\\n[DEBUG] Calculating returns...\")\n",
    "    df['returns'] = df.groupby('ticker')['close'].pct_change()\n",
    "    print(\"Sample returns:\")\n",
    "    print(df[['ticker', 'date', 'close', 'returns']].head())\n",
    "    \n",
    "    # Debug: Creating technical features\n",
    "    print(\"\\n[DEBUG] Creating technical features...\")\n",
    "    df['volume_ma5'] = df.groupby('ticker')['volume'].rolling(window=5).mean().reset_index(0, drop=True)\n",
    "    df['price_ma5'] = df.groupby('ticker')['close'].rolling(window=5).mean().reset_index(0, drop=True)\n",
    "    df['price_ma10'] = df.groupby('ticker')['close'].rolling(window=10).mean().reset_index(0, drop=True)\n",
    "    print(\"Sample technical features:\")\n",
    "    print(df[['ticker', 'date', 'volume_ma5', 'price_ma5', 'price_ma10']].head())\n",
    "\n",
    "    # Extracting PE and ROE\n",
    "    print(\"\\n[DEBUG] Extracting fundamental data...\")\n",
    "    def extract_pe_ratio(x):\n",
    "        if isinstance(x, float):\n",
    "            return x\n",
    "        if isinstance(x, dict) and x:\n",
    "            first_key = next(iter(x))\n",
    "            if isinstance(x[first_key], dict):\n",
    "                return x[first_key].get('pe_ratio', None)\n",
    "            return x[first_key]\n",
    "        return None\n",
    "\n",
    "    def extract_roe(x):\n",
    "        if isinstance(x, float):\n",
    "            return x\n",
    "        if isinstance(x, dict) and x:\n",
    "            first_key = next(iter(x))\n",
    "            if isinstance(x[first_key], dict):\n",
    "                return x[first_key].get('roe', None)\n",
    "            return x[first_key]\n",
    "        return None\n",
    "\n",
    "    df['pe'] = df['pe_ratio'].apply(extract_pe_ratio)\n",
    "    df['roe'] = df['pe_ratio'].apply(extract_roe)\n",
    "    df[['pe', 'roe']] = df.groupby('ticker')[['pe', 'roe']].ffill()\n",
    "\n",
    "    # Debug: PE and ROE values\n",
    "    print(\"Extracted PE and ROE (after forward-fill):\")\n",
    "    print(df[['ticker', 'date', 'pe', 'roe']].head(10))\n",
    "\n",
    "    # Calculate volatility and risk metrics\n",
    "    df['volatility'] = df.groupby('ticker')['returns'].transform(lambda x: x.rolling(window=30, min_periods=5).std())\n",
    "    market_returns = df[df['ticker'] == 'SPY']['returns']\n",
    "    def calculate_beta(stock_returns):\n",
    "        if len(stock_returns) < 5:\n",
    "            return pd.Series([None] * len(stock_returns))\n",
    "        rolling_cov = stock_returns.rolling(window=30, min_periods=5).cov(market_returns)\n",
    "        rolling_market_var = market_returns.rolling(window=30, min_periods=5).var()\n",
    "        return rolling_cov / rolling_market_var\n",
    "\n",
    "    df['beta'] = df.groupby('ticker')['returns'].transform(calculate_beta)\n",
    "    df['liquidity_ratio'] = df.groupby('ticker')['volume'].transform(lambda x: x / x.rolling(window=30, min_periods=5).mean())\n",
    "\n",
    "    # Debug: Risk metrics\n",
    "    print(\"Sample risk metrics:\")\n",
    "    print(df[['ticker', 'date', 'volatility', 'beta', 'liquidity_ratio']].head())\n",
    "\n",
    "    # Process news data\n",
    "    df['news_count'] = df['news'].apply(len)\n",
    "    print(\"\\n[DEBUG] Processing news data...\")\n",
    "    print(\"News counts:\")\n",
    "    print(df[['ticker', 'date', 'news_count']].head())\n",
    "\n",
    "    # Load macro data and filter\n",
    "    macro_df = load_cached_data(\"macro_uncertainty\", start_date=start_date, end_date=end_date)\n",
    "    if macro_df is None:\n",
    "        raise ValueError(\"Macro uncertainty data could not be loaded.\")\n",
    "    macro_start = macro_df.index[0]\n",
    "    print(f\"[DEBUG] Filtering out rows with date equal to macro start date: {macro_start}\")\n",
    "    df = df[df['date'] != macro_start]\n",
    "\n",
    "    # Initialize expert models\n",
    "    macro_expert = MacroExpert(macro_df=macro_df, fomcFetcher=FOMCDateBasedFetcher())\n",
    "    fundamental_expert = FundamentalExpert()\n",
    "    news_expert = NewsExpert()\n",
    "    technical_expert = TechnicalExpert()\n",
    "    risk_expert = RiskExpert()\n",
    "\n",
    "    # Collect records for training\n",
    "    records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\n[DEBUG] Processing {row['ticker']} {str(row['date'])}:\")\n",
    "        expert_predictions = []\n",
    "        macro_pred = macro_expert.produce_factor(row['date'], row['ticker'])\n",
    "        expert_predictions.append(macro_pred)\n",
    "        fund_data = {'pe': row['pe'], 'roe': row['roe']}\n",
    "        fund_pred = fundamental_expert.produce_factor(fund_data)\n",
    "        expert_predictions.append(fund_pred)\n",
    "        news_pred = news_expert.produce_factor(row['ticker'], row['date'])\n",
    "        expert_predictions.append(news_pred)\n",
    "        try:\n",
    "            tech_pred = technical_expert.produce_factor(df, idx)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] TechnicalExpert: {e} Returning default factor of 0.0.\")\n",
    "            tech_pred = 0.0\n",
    "        expert_predictions.append(tech_pred)\n",
    "        try:\n",
    "            risk_pred = risk_expert.produce_factor(df, idx)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] RiskExpert: {e} Returning default factor of 0.0.\")\n",
    "            risk_pred = 0.0\n",
    "        expert_predictions.append(risk_pred)\n",
    "\n",
    "        row_data = {\n",
    "            'ticker': row['ticker'],\n",
    "            'date': str(row['date']),\n",
    "            'macro_data': {\n",
    "                'uncertainty': row['macro_data'],\n",
    "                'fomc_statement': row['fomc']\n",
    "            },\n",
    "            'fundamental_data': {\n",
    "                'pe': float(row['pe']) if pd.notnull(row['pe']) else None,\n",
    "                'roe': float(row['roe']) if pd.notnull(row['roe']) else None\n",
    "            },\n",
    "            'technical_data': {\n",
    "                'current_price': float(row['close']),\n",
    "                'volume': int(row['volume']),\n",
    "                'price_ma5': float(row['price_ma5']) if pd.notnull(row['price_ma5']) else None,\n",
    "                'price_ma10': float(row['price_ma10']) if pd.notnull(row['price_ma10']) else None,\n",
    "                'volume_ma5': float(row['volume_ma5']) if pd.notnull(row['volume_ma5']) else None\n",
    "            },\n",
    "            'news_data': row['news'],\n",
    "            'risk_data': {\n",
    "                'volatility': float(row['volatility']) if pd.notnull(row['volatility']) else None,\n",
    "                'beta': float(row['beta']) if pd.notnull(row['beta']) else None,\n",
    "                'liquidity_ratio': float(row['liquidity_ratio']) if pd.notnull(row['liquidity_ratio']) else None\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Print the formatted data for debugging\n",
    "        print(\"\\n[DEBUG] Formatted data:\")\n",
    "        print(json.dumps(row_data, indent=2, default=str))\n",
    "\n",
    "        system_msg = (\n",
    "            \"You are a trading assistant that analyzes market data and provides a factor score between -1 and 1 indicating bullishness/bearishness.\\n\"\n",
    "            \"Your output must be exactly two lines: 'Factor: <float>' and 'Explanation: <text>'.\\n\"\n",
    "            \"Consider all available data including fundamentals, technicals, news, and macro factors.\"\n",
    "        )\n",
    "        user_msg = f\"\"\"Please analyze this data and provide a factor score between -1 (extremely bearish) and 1 (extremely bullish):\n",
    "{json.dumps(row_data, indent=2, default=str)}\"\"\"\n",
    "        single_pred, explanation = call_gpt_factor_and_expl(system_msg, user_msg)\n",
    "        record = {\n",
    "            'ticker': row['ticker'],\n",
    "            'date': row['date'],\n",
    "            'expert_predictions': expert_predictions,\n",
    "            'single_prediction': single_pred,\n",
    "            'explanation': explanation,\n",
    "            'target': row['returns'] if pd.notnull(row['returns']) else 0.0,\n",
    "            'true_return': row['returns'] if pd.notnull(row['returns']) else 0.0\n",
    "        }\n",
    "        records.append(record)\n",
    "        print(f\"Expert predictions: {expert_predictions}\")\n",
    "        print(f\"Single prediction: {single_pred}; Explanation: {explanation}.\")\n",
    "\n",
    "    # Train the MoE model\n",
    "    moe_model = train_moe_model(records)\n",
    "    for record in records:\n",
    "        expert_preds = torch.tensor(record['expert_predictions']).unsqueeze(0)\n",
    "        moe_pred = moe_model(expert_preds).item()\n",
    "        record['moe_prediction'] = moe_pred\n",
    "\n",
    "    # Compare the models and generate the portfolio comparison graph\n",
    "    compare_moe_vs_single(records)\n",
    "\n",
    "    # Portfolio Comparison: Best 20% vs Worst 20% Stocks\n",
    "    compare_portfolio_performance(records, df)\n",
    "\n",
    "    # Save prediction results to CSV\n",
    "    save_prediction_results(records, 'prediction_results.csv')\n",
    "\n",
    "    return records\n",
    "\n",
    "def compare_portfolio_performance(records: List[Dict], df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compare the MoE and GPT portfolios by buying the top 20% and shorting the bottom 20% \n",
    "    based on their factor predictions, and plotting the portfolio returns.\n",
    "    \"\"\"\n",
    "    # Add portfolio returns\n",
    "    df['moe_pred'] = df.apply(lambda row: next((r['moe_prediction'] for r in records if r['ticker'] == row['ticker'] and r['date'] == row['date']), 0.0), axis=1)\n",
    "    df['gpt_pred'] = df.apply(lambda row: next((r['single_prediction'] for r in records if r['ticker'] == row['ticker'] and r['date'] == row['date']), 0.0), axis=1)\n",
    "\n",
    "    # Sort by predictions to select best and worst 20% stocks\n",
    "    df['moe_rank'] = df['moe_pred'].rank(ascending=False)\n",
    "    df['gpt_rank'] = df['gpt_pred'].rank(ascending=False)\n",
    "    top_20_moe = df.nlargest(int(len(df) * 0.2), 'moe_rank')\n",
    "    bottom_20_moe = df.nsmallest(int(len(df) * 0.2), 'moe_rank')\n",
    "    top_20_gpt = df.nlargest(int(len(df) * 0.2), 'gpt_rank')\n",
    "    bottom_20_gpt = df.nsmallest(int(len(df) * 0.2), 'gpt_rank')\n",
    "\n",
    "    # Simulate portfolio returns\n",
    "    moe_returns = top_20_moe['returns'].mean() - bottom_20_moe['returns'].mean()\n",
    "    gpt_returns = top_20_gpt['returns'].mean() - bottom_20_gpt['returns'].mean()\n",
    "    sp500_returns = df[df['ticker'] == 'SPY']['returns'].mean()\n",
    "\n",
    "    # Plot the comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['MoE Portfolio', 'GPT Portfolio', 'S&P 500'], [moe_returns, gpt_returns, sp500_returns])\n",
    "    plt.title('Portfolio Performance Comparison (Best 20% / Worst 20%)')\n",
    "    plt.ylabel('Average Return')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_prediction_results(records: List[Dict], filename: str):\n",
    "    \"\"\"\n",
    "    Save the prediction results to a CSV file.\n",
    "    \"\"\"\n",
    "    # Create DataFrame from records\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"[INFO] Prediction results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_date = \"2024-12-18\"  # Explicit date range used for cache and processing\n",
    "    end_date = \"2025-01-16\"\n",
    "    print(f\"[INFO] Using date range: {start_date} to {end_date}\")\n",
    "    df = build_dataset(start_date=start_date, end_date=end_date)\n",
    "    records = run_model(df, start_date, end_date)\n",
    "    print(\"\\n[DEBUG] Final records sample:\")\n",
    "    for rec in records[:3]:\n",
    "        print(json.dumps(rec, indent=2, default=str))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
